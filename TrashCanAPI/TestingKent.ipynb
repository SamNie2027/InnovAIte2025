{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\krome\\OneDrive\\Desktop\\innovaite\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:208: UserWarning: The parameter 'pretrained' is deprecated since 0.13 and may be removed in the future, please use 'weights' instead.\n",
      "  warnings.warn(\n",
      "c:\\Users\\krome\\OneDrive\\Desktop\\innovaite\\.venv\\Lib\\site-packages\\torchvision\\models\\_utils.py:223: UserWarning: Arguments other than a weight enum or `None` for 'weights' are deprecated since 0.13 and may be removed in the future. The current behavior is equivalent to passing `weights=ResNet18_Weights.IMAGENET1K_V1`. You can also use `weights=ResNet18_Weights.DEFAULT` to get the most up-to-date weights.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "### Testing for updateTrashCanStatus\n",
    "from CompareImagesModel import *\n",
    "\n",
    "model = CompareImagesModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "def load_image_paths_from_folders_to_df(base_path):\n",
    "    '''\n",
    "    Provide base path for images and will return a pandas dataframe with the image paths and labels.\n",
    "    '''\n",
    "    image_paths = []\n",
    "    labels = []\n",
    "    label_names = os.listdir(base_path)\n",
    "\n",
    "    for label_idx, label_name in enumerate(label_names):\n",
    "        label_folder = os.path.join(base_path, label_name)\n",
    "\n",
    "        if os.path.isdir(label_folder):\n",
    "            for image_name in os.listdir(label_folder):\n",
    "                image_path = os.path.join(label_folder, image_name)\n",
    "\n",
    "                if image_path.endswith(('.png', '.jpg', '.jpeg')):  # Filter by image extensions\n",
    "                    image_paths.append(image_path)\n",
    "                    labels.append(label_name)  # Label is the name of the folder in label_names\n",
    "\n",
    "    df = pd.DataFrame({'IMAGEPATH': image_paths, 'ID': labels })\n",
    "\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "Cleaned = \"../../Trash Cans Cleaned\"  # Replace with your destination path\n",
    "trashcan_images_df = load_image_paths_from_folders_to_df(Cleaned)\n",
    "trashcan_images_df['ID'] = trashcan_images_df['ID'].astype(int)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1, 16, 17, 18, 19,  2, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29,  3,\n",
       "       30, 31, 32, 33, 38, 39,  4,  5,  6,  7])"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trashcan_images_df[\"ID\"].unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "0.84914136\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "# Load the baseline image and extract its features\n",
    "trashcan_id = 21\n",
    "# get all image paths for trashcan of trashcan_id\n",
    "filtered_paths = trashcan_images_df[trashcan_images_df['ID'] == trashcan_id][\"IMAGEPATH\"].values\n",
    "print(type(filtered_paths))\n",
    "\n",
    "baseline_img = Image.open(filtered_paths[0])\n",
    "other_img = Image.open(filtered_paths[1])\n",
    "\n",
    "print(model.predict(baseline_img, other_img))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[20]\n",
      "['../../Trash Cans Cleaned\\\\20']\n",
      "Expected: 20 Returned: 20\n"
     ]
    }
   ],
   "source": [
    "from updateTrashCanStatus import matchTrashCanID\n",
    "trash_can_ids = [20] #[19, 20, 21, 22]\n",
    "image_folder_paths = []\n",
    "for id in trash_can_ids:\n",
    "    paths = trashcan_images_df[trashcan_images_df['ID'] == id][\"IMAGEPATH\"].values\n",
    "    image_folder_paths.append(os.path.dirname(paths[0]))\n",
    "\n",
    "trashcan_id = 20\n",
    "filtered_paths = trashcan_images_df[trashcan_images_df['ID'] == trashcan_id][\"IMAGEPATH\"].values\n",
    "image = Image.open(filtered_paths[0])\n",
    "\n",
    "# pred_id = matchTrashCanID(image, image_folder_paths, trash_can_ids, model)\n",
    "\n",
    "# print(trash_can_ids)\n",
    "# print(image_folder_paths)\n",
    "# print(\"Expected:\", trashcan_id, \"Returned:\", pred_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min lat: 42.338382800000005 max_lat: 42.3403828\n",
      "min long: -71.0881835 max long: -71.08618349999999\n",
      "image_folder_paths: ['../Trash Cans Cleaned/1', '../Trash Cans Cleaned/2', '../Trash Cans Cleaned/3', '../Trash Cans Cleaned/16', '../Trash Cans Cleaned/17', '../Trash Cans Cleaned/18', '../Trash Cans Cleaned/20', '../Trash Cans Cleaned/38', '../Trash Cans Cleaned/39']\n",
      "IDs: [1, 2, 3, 16, 17, 18, 20, 38, 39]\n",
      "ID: 20\n",
      "Marking trash can 20 as FULL.\n"
     ]
    }
   ],
   "source": [
    "import updateTrashCanStatus\n",
    "import importlib\n",
    "importlib.reload(updateTrashCanStatus)\n",
    "\n",
    "latitude, longitude = 42.3393828, -71.0871835 #42, -71\n",
    "updateTrashCanStatus.updateTrashCanStatusMain(image=image, latitude=latitude, longitude=longitude,full_likelihood=1, model=model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sqlite3\n",
    "\n",
    "# Create (or connect to) the database\n",
    "conn = sqlite3.connect(\"database.db\")\n",
    "cursor = conn.cursor()\n",
    "\n",
    "# Read the .sql file\n",
    "with open(\"trash_can.sql\", \"r\") as f:\n",
    "    sql_script = f.read()\n",
    "\n",
    "# Execute the SQL script\n",
    "cursor.executescript(sql_script)\n",
    "\n",
    "# Commit and close\n",
    "conn.commit()\n",
    "conn.close()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Marking trash can 19 as FULL.\n",
      "\n",
      "Database Output:\n",
      "(19, 42.338979, -71.0885834, 1, '2025-02-16 07:21:28')\n"
     ]
    }
   ],
   "source": [
    "from updateTrashCanStatus import updateTrashCanStatus\n",
    "DATABASE_PATH = \"database.db\"\n",
    "import sqlite3\n",
    "\n",
    "# Call function after defining config\n",
    "trash_can_id = 19\n",
    "updateTrashCanStatus(trash_can_id, 1)\n",
    "\n",
    "# Connect to database and verify result\n",
    "try:\n",
    "    connection = sqlite3.connect(DATABASE_PATH)\n",
    "    cursor = connection.cursor()\n",
    "\n",
    "    # Query the updated row\n",
    "    cursor.execute(f\"SELECT * FROM trash_cans WHERE trash_can_id = {trash_can_id};\")\n",
    "    result = cursor.fetchone()\n",
    "\n",
    "    print(\"\\nDatabase Output:\")\n",
    "    print(result)  # This should print the updated trash can row\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Database Error: {e}\")\n",
    "\n",
    "finally:\n",
    "    if cursor:\n",
    "        cursor.close()\n",
    "    if connection:\n",
    "        connection.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "min lat: 42.338382800000005 max_lat: 42.3403828\n",
      "min long: -71.0881835 max long: -71.08618349999999\n",
      "[1, 2, 3, 16, 17, 18, 20, 38, 39]\n",
      "[['Trash Cans Cleaned/1/'], ['Trash Cans Cleaned/2/'], ['Trash Cans Cleaned/3/'], ['Trash Cans Cleaned/16/'], ['Trash Cans Cleaned/17/'], ['Trash Cans Cleaned/18/'], ['Trash Cans Cleaned/20/'], ['Trash Cans Cleaned/38/'], ['Trash Cans Cleaned/39/']]\n"
     ]
    }
   ],
   "source": [
    "from updateTrashCanStatus import getNearbyTrashCanImages\n",
    "latitude, longitude = 42.3393828, -71.0871835 #42, -71\n",
    "image_folder_paths, trash_can_ids = getNearbyTrashCanImages(latitude, longitude, range=0.001)\n",
    "print(trash_can_ids)\n",
    "print(image_folder_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FullnessModel import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "FullnessModel.__init__() missing 2 required positional arguments: 'train_dir' and 'validate_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m fullness_model \u001b[38;5;241m=\u001b[39m \u001b[43mFullnessModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mTypeError\u001b[0m: FullnessModel.__init__() missing 2 required positional arguments: 'train_dir' and 'validate_dir'"
     ]
    }
   ],
   "source": [
    "fullness_model = FullnessModel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow\n",
      "  Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl.metadata (3.3 kB)\n",
      "Collecting tensorflow-intel==2.18.0 (from tensorflow)\n",
      "  Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl.metadata (4.9 kB)\n",
      "Collecting absl-py>=1.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached absl_py-2.1.0-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting astunparse>=1.6.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached astunparse-1.6.3-py2.py3-none-any.whl.metadata (4.4 kB)\n",
      "Collecting flatbuffers>=24.3.25 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading flatbuffers-25.2.10-py2.py3-none-any.whl.metadata (875 bytes)\n",
      "Collecting gast!=0.5.0,!=0.5.1,!=0.5.2,>=0.2.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached gast-0.6.0-py3-none-any.whl.metadata (1.3 kB)\n",
      "Collecting google-pasta>=0.1.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached google_pasta-0.2.0-py3-none-any.whl.metadata (814 bytes)\n",
      "Collecting libclang>=13.0.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl.metadata (5.3 kB)\n",
      "Collecting opt-einsum>=2.3.2 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached opt_einsum-3.4.0-py3-none-any.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: packaging in c:\\users\\krome\\onedrive\\desktop\\innovaite\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (24.2)\n",
      "Collecting protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl.metadata (592 bytes)\n",
      "Collecting requests<3,>=2.21.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached requests-2.32.3-py3-none-any.whl.metadata (4.6 kB)\n",
      "Requirement already satisfied: setuptools in c:\\users\\krome\\onedrive\\desktop\\innovaite\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (65.5.0)\n",
      "Requirement already satisfied: six>=1.12.0 in c:\\users\\krome\\onedrive\\desktop\\innovaite\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (1.17.0)\n",
      "Collecting termcolor>=1.1.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached termcolor-2.5.0-py3-none-any.whl.metadata (6.1 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in c:\\users\\krome\\onedrive\\desktop\\innovaite\\.venv\\lib\\site-packages (from tensorflow-intel==2.18.0->tensorflow) (4.12.2)\n",
      "Collecting wrapt>=1.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl.metadata (6.5 kB)\n",
      "Collecting grpcio<2.0,>=1.24.3 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached grpcio-1.70.0-cp311-cp311-win_amd64.whl.metadata (4.0 kB)\n",
      "Collecting tensorboard<2.19,>=2.18 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard-2.18.0-py3-none-any.whl.metadata (1.6 kB)\n",
      "Collecting keras>=3.5.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached keras-3.8.0-py3-none-any.whl.metadata (5.8 kB)\n",
      "Collecting numpy<2.1.0,>=1.26.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached numpy-2.0.2-cp311-cp311-win_amd64.whl.metadata (59 kB)\n",
      "Collecting h5py>=3.11.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached h5py-3.12.1-cp311-cp311-win_amd64.whl.metadata (2.5 kB)\n",
      "Collecting ml-dtypes<0.5.0,>=0.4.0 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl.metadata (20 kB)\n",
      "Collecting tensorflow-io-gcs-filesystem>=0.23.1 (from tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl.metadata (14 kB)\n",
      "Collecting wheel<1.0,>=0.23.0 (from astunparse>=1.6.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached wheel-0.45.1-py3-none-any.whl.metadata (2.3 kB)\n",
      "Collecting rich (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached rich-13.9.4-py3-none-any.whl.metadata (18 kB)\n",
      "Collecting namex (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached namex-0.0.8-py3-none-any.whl.metadata (246 bytes)\n",
      "Collecting optree (from keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached optree-0.14.0-cp311-cp311-win_amd64.whl.metadata (48 kB)\n",
      "Collecting charset-normalizer<4,>=2 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl.metadata (36 kB)\n",
      "Collecting idna<4,>=2.5 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached idna-3.10-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting urllib3<3,>=1.21.1 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached urllib3-2.3.0-py3-none-any.whl.metadata (6.5 kB)\n",
      "Collecting certifi>=2017.4.17 (from requests<3,>=2.21.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Downloading certifi-2025.1.31-py3-none-any.whl.metadata (2.5 kB)\n",
      "Collecting markdown>=2.6.8 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached Markdown-3.7-py3-none-any.whl.metadata (7.0 kB)\n",
      "Collecting tensorboard-data-server<0.8.0,>=0.7.0 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached tensorboard_data_server-0.7.2-py3-none-any.whl.metadata (1.1 kB)\n",
      "Collecting werkzeug>=1.0.1 (from tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached werkzeug-3.1.3-py3-none-any.whl.metadata (3.7 kB)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in c:\\users\\krome\\onedrive\\desktop\\innovaite\\.venv\\lib\\site-packages (from werkzeug>=1.0.1->tensorboard<2.19,>=2.18->tensorflow-intel==2.18.0->tensorflow) (3.0.2)\n",
      "Collecting markdown-it-py>=2.2.0 (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached markdown_it_py-3.0.0-py3-none-any.whl.metadata (6.9 kB)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in c:\\users\\krome\\onedrive\\desktop\\innovaite\\.venv\\lib\\site-packages (from rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow) (2.19.1)\n",
      "Collecting mdurl~=0.1 (from markdown-it-py>=2.2.0->rich->keras>=3.5.0->tensorflow-intel==2.18.0->tensorflow)\n",
      "  Using cached mdurl-0.1.2-py3-none-any.whl.metadata (1.6 kB)\n",
      "Using cached tensorflow-2.18.0-cp311-cp311-win_amd64.whl (7.5 kB)\n",
      "Using cached tensorflow_intel-2.18.0-cp311-cp311-win_amd64.whl (390.2 MB)\n",
      "Using cached absl_py-2.1.0-py3-none-any.whl (133 kB)\n",
      "Using cached astunparse-1.6.3-py2.py3-none-any.whl (12 kB)\n",
      "Downloading flatbuffers-25.2.10-py2.py3-none-any.whl (30 kB)\n",
      "Using cached gast-0.6.0-py3-none-any.whl (21 kB)\n",
      "Using cached google_pasta-0.2.0-py3-none-any.whl (57 kB)\n",
      "Using cached grpcio-1.70.0-cp311-cp311-win_amd64.whl (4.3 MB)\n",
      "Using cached h5py-3.12.1-cp311-cp311-win_amd64.whl (3.0 MB)\n",
      "Using cached keras-3.8.0-py3-none-any.whl (1.3 MB)\n",
      "Using cached libclang-18.1.1-py2.py3-none-win_amd64.whl (26.4 MB)\n",
      "Using cached ml_dtypes-0.4.1-cp311-cp311-win_amd64.whl (126 kB)\n",
      "Using cached numpy-2.0.2-cp311-cp311-win_amd64.whl (15.9 MB)\n",
      "Using cached opt_einsum-3.4.0-py3-none-any.whl (71 kB)\n",
      "Using cached protobuf-5.29.3-cp310-abi3-win_amd64.whl (434 kB)\n",
      "Using cached requests-2.32.3-py3-none-any.whl (64 kB)\n",
      "Using cached tensorboard-2.18.0-py3-none-any.whl (5.5 MB)\n",
      "Using cached tensorflow_io_gcs_filesystem-0.31.0-cp311-cp311-win_amd64.whl (1.5 MB)\n",
      "Using cached termcolor-2.5.0-py3-none-any.whl (7.8 kB)\n",
      "Using cached wrapt-1.17.2-cp311-cp311-win_amd64.whl (38 kB)\n",
      "Downloading certifi-2025.1.31-py3-none-any.whl (166 kB)\n",
      "Using cached charset_normalizer-3.4.1-cp311-cp311-win_amd64.whl (102 kB)\n",
      "Using cached idna-3.10-py3-none-any.whl (70 kB)\n",
      "Using cached Markdown-3.7-py3-none-any.whl (106 kB)\n",
      "Using cached tensorboard_data_server-0.7.2-py3-none-any.whl (2.4 kB)\n",
      "Using cached urllib3-2.3.0-py3-none-any.whl (128 kB)\n",
      "Using cached werkzeug-3.1.3-py3-none-any.whl (224 kB)\n",
      "Using cached wheel-0.45.1-py3-none-any.whl (72 kB)\n",
      "Using cached namex-0.0.8-py3-none-any.whl (5.8 kB)\n",
      "Using cached optree-0.14.0-cp311-cp311-win_amd64.whl (300 kB)\n",
      "Using cached rich-13.9.4-py3-none-any.whl (242 kB)\n",
      "Using cached markdown_it_py-3.0.0-py3-none-any.whl (87 kB)\n",
      "Using cached mdurl-0.1.2-py3-none-any.whl (10.0 kB)\n",
      "Installing collected packages: namex, libclang, flatbuffers, wrapt, wheel, werkzeug, urllib3, termcolor, tensorflow-io-gcs-filesystem, tensorboard-data-server, protobuf, optree, opt-einsum, numpy, mdurl, markdown, idna, grpcio, google-pasta, gast, charset-normalizer, certifi, absl-py, tensorboard, requests, ml-dtypes, markdown-it-py, h5py, astunparse, rich, keras, tensorflow-intel, tensorflow\n",
      "  Attempting uninstall: numpy\n",
      "    Found existing installation: numpy 2.2.3\n",
      "    Uninstalling numpy-2.2.3:\n",
      "      Successfully uninstalled numpy-2.2.3\n",
      "Successfully installed absl-py-2.1.0 astunparse-1.6.3 certifi-2025.1.31 charset-normalizer-3.4.1 flatbuffers-25.2.10 gast-0.6.0 google-pasta-0.2.0 grpcio-1.70.0 h5py-3.12.1 idna-3.10 keras-3.8.0 libclang-18.1.1 markdown-3.7 markdown-it-py-3.0.0 mdurl-0.1.2 ml-dtypes-0.4.1 namex-0.0.8 numpy-2.0.2 opt-einsum-3.4.0 optree-0.14.0 protobuf-5.29.3 requests-2.32.3 rich-13.9.4 tensorboard-2.18.0 tensorboard-data-server-0.7.2 tensorflow-2.18.0 tensorflow-intel-2.18.0 tensorflow-io-gcs-filesystem-0.31.0 termcolor-2.5.0 urllib3-2.3.0 werkzeug-3.1.3 wheel-0.45.1 wrapt-1.17.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\krome\\OneDrive\\Desktop\\innovaite\\.venv\\Lib\\site-packages\\~umpy.libs'.\n",
      "  You can safely remove it manually.\n",
      "  WARNING: Failed to remove contents in a temporary directory 'C:\\Users\\krome\\OneDrive\\Desktop\\innovaite\\.venv\\Lib\\site-packages\\~umpy'.\n",
      "  You can safely remove it manually.\n"
     ]
    }
   ],
   "source": [
    "! pip install tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
